{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":27968,"databundleVersionId":2576072,"sourceType":"competition"}],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch\nimport random\nfrom torch import nn\nimport pandas as pd\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision.io import read_image\nfrom torchvision.models import resnet18\nfrom torchvision.models import ResNet18_Weights\nfrom torchvision import transforms\nfrom torchvision.transforms import v2\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-09T11:00:41.281056Z","iopub.execute_input":"2024-12-09T11:00:41.281392Z","iopub.status.idle":"2024-12-09T11:00:46.054632Z","shell.execute_reply.started":"2024-12-09T11:00:41.281351Z","shell.execute_reply":"2024-12-09T11:00:46.053725Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"PATH = '/kaggle/input/aaa-ml/avito-auto-moderation'\nTRAIN_FILE = 'train_v2.csv'\nTEST_FILE = 'sample_submission_v2.csv'\nDEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nRANDOM_STATE = 42\nBATCH_SIZE = 16","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T11:00:48.231408Z","iopub.execute_input":"2024-12-09T11:00:48.231904Z","iopub.status.idle":"2024-12-09T11:00:48.302651Z","shell.execute_reply.started":"2024-12-09T11:00:48.231870Z","shell.execute_reply":"2024-12-09T11:00:48.301730Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"data = pd.read_csv(os.path.join(PATH, TRAIN_FILE))\ndata.head(5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T11:00:50.686230Z","iopub.execute_input":"2024-12-09T11:00:50.686569Z","iopub.status.idle":"2024-12-09T11:00:50.715073Z","shell.execute_reply.started":"2024-12-09T11:00:50.686538Z","shell.execute_reply":"2024-12-09T11:00:50.714194Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"      image  label\n0  1798.jpg      0\n1   372.jpg      1\n2   124.jpg      0\n3  1155.jpg      0\n4  1227.jpg      0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1798.jpg</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>372.jpg</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>124.jpg</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1155.jpg</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1227.jpg</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"data.isna().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T11:00:53.967556Z","iopub.execute_input":"2024-12-09T11:00:53.967898Z","iopub.status.idle":"2024-12-09T11:00:53.976991Z","shell.execute_reply.started":"2024-12-09T11:00:53.967868Z","shell.execute_reply":"2024-12-09T11:00:53.976022Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"image    0\nlabel    0\ndtype: int64"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"data.groupby('label').count()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T11:00:56.286085Z","iopub.execute_input":"2024-12-09T11:00:56.286416Z","iopub.status.idle":"2024-12-09T11:00:56.301474Z","shell.execute_reply.started":"2024-12-09T11:00:56.286386Z","shell.execute_reply":"2024-12-09T11:00:56.300629Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"       image\nlabel       \n0        971\n1        172","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image</th>\n    </tr>\n    <tr>\n      <th>label</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>971</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>172</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"'''shape_1 = dict()\nshape_2 = dict()\nfor i in data['image']:\n    img = read_image(os.path.join(PATH, i))\n    shape_1[img.shape[1]] = shape_1.get(img.shape[1], 0) + 1\n    shape_2[img.shape[2]] = shape_2.get(img.shape[2], 0) + 1\nsize_1 = max(shape_1, key=shape_1.get)\nsize_2 = max(shape_2, key=shape_2.get)'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T10:54:13.690405Z","iopub.execute_input":"2024-12-09T10:54:13.691192Z","iopub.status.idle":"2024-12-09T10:54:13.696546Z","shell.execute_reply.started":"2024-12-09T10:54:13.691159Z","shell.execute_reply":"2024-12-09T10:54:13.695665Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"\"shape_1 = dict()\\nshape_2 = dict()\\nfor i in data['image']:\\n    img = read_image(os.path.join(PATH, i))\\n    shape_1[img.shape[1]] = shape_1.get(img.shape[1], 0) + 1\\n    shape_2[img.shape[2]] = shape_2.get(img.shape[2], 0) + 1\\nsize_1 = max(shape_1, key=shape_1.get)\\nsize_2 = max(shape_2, key=shape_2.get)\""},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(data['image'], data['label'], test_size=0.2, random_state=RANDOM_STATE, stratify=data['label'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T11:00:59.960269Z","iopub.execute_input":"2024-12-09T11:00:59.960607Z","iopub.status.idle":"2024-12-09T11:00:59.968975Z","shell.execute_reply.started":"2024-12-09T11:00:59.960577Z","shell.execute_reply":"2024-12-09T11:00:59.968299Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"class NormalizeImage:\n    def __call__(self, image):\n        return image.float() / 255.0\n\nMEAN = np.array([0.485, 0.456, 0.406])\nSTD = np.array([0.229, 0.224, 0.225])\n\ntrain_transform = transforms.Compose([\n    transforms.Resize((256, 256)),\n    NormalizeImage(),\n    transforms.Normalize(mean=MEAN, std=STD),\n    v2.GaussianNoise(),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomVerticalFlip(),\n    transforms.RandomRotation(20),\n])\n\ntest_transform = transforms.Compose([\n    transforms.Resize((256, 256)),\n    NormalizeImage(),\n    transforms.Normalize(mean=MEAN, std=STD),\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T11:02:04.689058Z","iopub.execute_input":"2024-12-09T11:02:04.689863Z","iopub.status.idle":"2024-12-09T11:02:04.696009Z","shell.execute_reply.started":"2024-12-09T11:02:04.689827Z","shell.execute_reply":"2024-12-09T11:02:04.695169Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"class ImageDataset(Dataset):\n    def __init__(self, images, labels, folder_path, target_len=None, transformation=None):\n        self.folder_path = folder_path\n        self.images = images\n        self.labels = labels\n        self.target_len = target_len\n        self.transformation = transformation\n\n    def __len__(self):\n        if self.target_len:\n            return self.target_len\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        if self.target_len:\n            idx = random.randint(0, len(self.labels) - 1)\n        image = read_image(os.path.join(self.folder_path, self.images[idx]))\n        if self.transformation:\n            image = self.transformation(image)\n        label = self.labels[idx]\n        return image, label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T11:02:08.861037Z","iopub.execute_input":"2024-12-09T11:02:08.861918Z","iopub.status.idle":"2024-12-09T11:02:08.867924Z","shell.execute_reply.started":"2024-12-09T11:02:08.861881Z","shell.execute_reply":"2024-12-09T11:02:08.866845Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"train_dataset = ImageDataset(X_train.values, y_train.values, PATH, transformation=train_transform)\ntest_dataset = ImageDataset(X_test.values, y_test.values, PATH, transformation=test_transform)\ntrain_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\ntest_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T11:02:13.678651Z","iopub.execute_input":"2024-12-09T11:02:13.679013Z","iopub.status.idle":"2024-12-09T11:02:13.683875Z","shell.execute_reply.started":"2024-12-09T11:02:13.678984Z","shell.execute_reply":"2024-12-09T11:02:13.682984Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"class CustomModel(nn.Module):\n    def __init__(self, num_classes):\n        super().__init__()\n        self.resnet = resnet18(weights=ResNet18_Weights.DEFAULT)\n        linear = nn.Linear(in_features=self.resnet.fc.in_features, out_features=num_classes, bias=True)\n        self.resnet.fc = linear\n        self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, x):\n        x = self.resnet(x)\n        x = self.softmax(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T11:02:17.484988Z","iopub.execute_input":"2024-12-09T11:02:17.485325Z","iopub.status.idle":"2024-12-09T11:02:17.490902Z","shell.execute_reply.started":"2024-12-09T11:02:17.485291Z","shell.execute_reply":"2024-12-09T11:02:17.489879Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"model = CustomModel(num_classes=2)\nmodel = model.to(DEVICE)\noptimizer = torch.optim.SGD(model.parameters(), lr = 0.001, momentum=0.9)\nweights = torch.tensor([1.0, 5.5]).to(DEVICE)\nloss_fn = nn.CrossEntropyLoss(weight=weights)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T11:02:20.486569Z","iopub.execute_input":"2024-12-09T11:02:20.486970Z","iopub.status.idle":"2024-12-09T11:02:21.256054Z","shell.execute_reply.started":"2024-12-09T11:02:20.486935Z","shell.execute_reply":"2024-12-09T11:02:21.255255Z"}},"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n100%|██████████| 44.7M/44.7M [00:00<00:00, 181MB/s]\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"def train_epoch(model, loss_fn, optimizer, dataloader):\n    model.train()\n    sum_loss = 0\n    for i, (x, y) in enumerate(dataloader):\n        x = x.to(DEVICE)\n        y = y.to(DEVICE)\n\n        preds = model(x)\n        loss = loss_fn(preds, y)\n        sum_loss += loss.item()\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    return sum_loss / len(dataloader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T11:02:23.776641Z","iopub.execute_input":"2024-12-09T11:02:23.777177Z","iopub.status.idle":"2024-12-09T11:02:23.785155Z","shell.execute_reply.started":"2024-12-09T11:02:23.777119Z","shell.execute_reply":"2024-12-09T11:02:23.784099Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"def eval_epoch(model, loss_fn, dataloader):\n    model.eval()\n    sum_loss = 0\n    y_true = []\n    y_pred = []\n    with torch.no_grad():\n        for i, (x, y) in enumerate(dataloader):\n            x = x.to(DEVICE)\n            y = y.to(DEVICE)\n\n            preds = model(x)\n            loss = loss_fn(preds, y)\n            sum_loss += loss.item()\n\n            y_true = y_true + y.tolist()\n            y_pred = y_pred + preds[:, 1].tolist()\n    return sum_loss / len(dataloader), roc_auc_score(y_true, y_pred)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T11:02:26.614900Z","iopub.execute_input":"2024-12-09T11:02:26.615739Z","iopub.status.idle":"2024-12-09T11:02:26.620928Z","shell.execute_reply.started":"2024-12-09T11:02:26.615707Z","shell.execute_reply":"2024-12-09T11:02:26.619954Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"epochs = 30\nfor i in range(epochs):\n    train_loss = train_epoch(model, loss_fn, optimizer, train_dataloader)\n    test_loss, roc_auc = eval_epoch(model, loss_fn, test_dataloader)\n\n    print(f'Epoch: {i}')\n    print(f'Train loss: {train_loss}')\n    print(f'Test loss: {test_loss}')\n    print(f'ROC AUC: {roc_auc}')\n    print('-----------------------------')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T11:02:30.009647Z","iopub.execute_input":"2024-12-09T11:02:30.010466Z","iopub.status.idle":"2024-12-09T11:09:14.818991Z","shell.execute_reply.started":"2024-12-09T11:02:30.010433Z","shell.execute_reply":"2024-12-09T11:09:14.817459Z"}},"outputs":[{"name":"stdout","text":"Epoch: 0\nTrain loss: 0.6414627278673237\nTest loss: 0.7374277949333191\nROC AUC: 0.5337858220211161\n-----------------------------\nEpoch: 1\nTrain loss: 0.5630690049508522\nTest loss: 0.682299796740214\nROC AUC: 0.5840120663650076\n-----------------------------\nEpoch: 2\nTrain loss: 0.5209482255680807\nTest loss: 0.7002211610476176\nROC AUC: 0.6351432880844646\n-----------------------------\nEpoch: 3\nTrain loss: 0.49295661110302497\nTest loss: 0.6585925777753194\nROC AUC: 0.6135746606334842\n-----------------------------\nEpoch: 4\nTrain loss: 0.480744157885683\nTest loss: 0.6592238108317058\nROC AUC: 0.6313725490196078\n-----------------------------\nEpoch: 5\nTrain loss: 0.44663636797461015\nTest loss: 0.7038535912831624\nROC AUC: 0.6143288084464554\n-----------------------------\nEpoch: 6\nTrain loss: 0.4565183900553605\nTest loss: 0.6693888187408448\nROC AUC: 0.6235294117647059\n-----------------------------\nEpoch: 7\nTrain loss: 0.4381017618138215\nTest loss: 0.6912757734457652\nROC AUC: 0.6054298642533937\n-----------------------------\nEpoch: 8\nTrain loss: 0.4368008179911252\nTest loss: 0.722900120417277\nROC AUC: 0.5619909502262443\n-----------------------------\nEpoch: 9\nTrain loss: 0.4177420714805866\nTest loss: 0.7162146468957266\nROC AUC: 0.6028657616892911\n-----------------------------\nEpoch: 10\nTrain loss: 0.4086499085714077\nTest loss: 0.6696673274040222\nROC AUC: 0.5835595776772248\n-----------------------------\nEpoch: 11\nTrain loss: 0.4024540925848073\nTest loss: 0.7013096213340759\nROC AUC: 0.5825037707390649\n-----------------------------\nEpoch: 12\nTrain loss: 0.38690789095286665\nTest loss: 0.715261165301005\nROC AUC: 0.5656108597285068\n-----------------------------\nEpoch: 13\nTrain loss: 0.3891083885883463\nTest loss: 0.7126884301503499\nROC AUC: 0.6007541478129713\n-----------------------------\nEpoch: 14\nTrain loss: 0.37637289844710253\nTest loss: 0.7189438283443451\nROC AUC: 0.5802413273001508\n-----------------------------\nEpoch: 15\nTrain loss: 0.3854314361152978\nTest loss: 0.7016492307186126\nROC AUC: 0.597737556561086\n-----------------------------\nEpoch: 16\nTrain loss: 0.38551846917333277\nTest loss: 0.7508588790893554\nROC AUC: 0.5631975867269985\n-----------------------------\nEpoch: 17\nTrain loss: 0.36426000605369435\nTest loss: 0.7550617178281148\nROC AUC: 0.5963800904977377\n-----------------------------\nEpoch: 18\nTrain loss: 0.3614370869151477\nTest loss: 0.7177835603555044\nROC AUC: 0.600603318250377\n-----------------------------\nEpoch: 19\nTrain loss: 0.3512156086749044\nTest loss: 0.7736186544100444\nROC AUC: 0.610708898944193\n-----------------------------\nEpoch: 20\nTrain loss: 0.36483904256902894\nTest loss: 0.7235442280769349\nROC AUC: 0.6034690799396681\n-----------------------------\nEpoch: 21\nTrain loss: 0.3574032197738516\nTest loss: 0.7644845287005106\nROC AUC: 0.6025641025641025\n-----------------------------\nEpoch: 22\nTrain loss: 0.37409211952110816\nTest loss: 0.7629361589749654\nROC AUC: 0.6150829562594268\n-----------------------------\nEpoch: 23\nTrain loss: 0.3545567896859399\nTest loss: 0.755581013361613\nROC AUC: 0.6208144796380091\n-----------------------------\nEpoch: 24\nTrain loss: 0.35092613717605325\nTest loss: 0.7589648922284444\nROC AUC: 0.6191553544494721\n-----------------------------\nEpoch: 25\nTrain loss: 0.3507855180008658\nTest loss: 0.7046407381693522\nROC AUC: 0.6684766214177978\n-----------------------------\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[14], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m30\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m----> 3\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     test_loss, roc_auc \u001b[38;5;241m=\u001b[39m eval_epoch(model, loss_fn, test_dataloader)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n","Cell \u001b[0;32mIn[12], line 4\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, loss_fn, optimizer, dataloader)\u001b[0m\n\u001b[1;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      3\u001b[0m sum_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (x, y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[1;32m      5\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m      6\u001b[0m     y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mto(DEVICE)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","Cell \u001b[0;32mIn[8], line 19\u001b[0m, in \u001b[0;36mImageDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     17\u001b[0m image \u001b[38;5;241m=\u001b[39m read_image(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfolder_path, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimages[idx]))\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformation:\n\u001b[0;32m---> 19\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels[idx]\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m image, label\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/transforms/v2/_transform.py:50\u001b[0m, in \u001b[0;36mTransform.forward\u001b[0;34m(self, *inputs)\u001b[0m\n\u001b[1;32m     45\u001b[0m needs_transform_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_needs_transform_list(flat_inputs)\n\u001b[1;32m     46\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_params(\n\u001b[1;32m     47\u001b[0m     [inpt \u001b[38;5;28;01mfor\u001b[39;00m (inpt, needs_transform) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(flat_inputs, needs_transform_list) \u001b[38;5;28;01mif\u001b[39;00m needs_transform]\n\u001b[1;32m     48\u001b[0m )\n\u001b[0;32m---> 50\u001b[0m flat_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform(inpt, params) \u001b[38;5;28;01mif\u001b[39;00m needs_transform \u001b[38;5;28;01melse\u001b[39;00m inpt\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (inpt, needs_transform) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(flat_inputs, needs_transform_list)\n\u001b[1;32m     53\u001b[0m ]\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tree_unflatten(flat_outputs, spec)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/transforms/v2/_transform.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     45\u001b[0m needs_transform_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_needs_transform_list(flat_inputs)\n\u001b[1;32m     46\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_params(\n\u001b[1;32m     47\u001b[0m     [inpt \u001b[38;5;28;01mfor\u001b[39;00m (inpt, needs_transform) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(flat_inputs, needs_transform_list) \u001b[38;5;28;01mif\u001b[39;00m needs_transform]\n\u001b[1;32m     48\u001b[0m )\n\u001b[1;32m     50\u001b[0m flat_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43minpt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m needs_transform \u001b[38;5;28;01melse\u001b[39;00m inpt\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (inpt, needs_transform) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(flat_inputs, needs_transform_list)\n\u001b[1;32m     53\u001b[0m ]\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tree_unflatten(flat_outputs, spec)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/transforms/v2/_misc.py:232\u001b[0m, in \u001b[0;36mGaussianNoise._transform\u001b[0;34m(self, inpt, params)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, inpt: Any, params: Dict[\u001b[38;5;28mstr\u001b[39m, Any]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m--> 232\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_kernel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgaussian_noise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minpt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msigma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclip\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/transforms/v2/_transform.py:35\u001b[0m, in \u001b[0;36mTransform._call_kernel\u001b[0;34m(self, functional, inpt, *args, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call_kernel\u001b[39m(\u001b[38;5;28mself\u001b[39m, functional: Callable, inpt: Any, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m     34\u001b[0m     kernel \u001b[38;5;241m=\u001b[39m _get_kernel(functional, \u001b[38;5;28mtype\u001b[39m(inpt), allow_passthrough\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mkernel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minpt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/transforms/v2/functional/_misc.py:203\u001b[0m, in \u001b[0;36mgaussian_noise_image\u001b[0;34m(image, mean, sigma, clip)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sigma \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msigma shouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be negative. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msigma\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 203\u001b[0m noise \u001b[38;5;241m=\u001b[39m mean \u001b[38;5;241m+\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandn_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m sigma\n\u001b[1;32m    204\u001b[0m out \u001b[38;5;241m=\u001b[39m image \u001b[38;5;241m+\u001b[39m noise\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m clip:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":14},{"cell_type":"markdown","source":"# Submit","metadata":{}},{"cell_type":"code","source":"sub_data = pd.read_csv(os.path.join(PATH, TEST_FILE))\nsub_data.head(5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T08:57:12.137515Z","iopub.execute_input":"2024-12-09T08:57:12.138090Z","iopub.status.idle":"2024-12-09T08:57:12.153445Z","shell.execute_reply.started":"2024-12-09T08:57:12.138056Z","shell.execute_reply":"2024-12-09T08:57:12.152754Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"      image  score\n0   474.jpg    0.5\n1  1052.jpg    0.5\n2    63.jpg    0.5\n3  1713.jpg    0.5\n4   116.jpg    0.5","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image</th>\n      <th>score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>474.jpg</td>\n      <td>0.5</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1052.jpg</td>\n      <td>0.5</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>63.jpg</td>\n      <td>0.5</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1713.jpg</td>\n      <td>0.5</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>116.jpg</td>\n      <td>0.5</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"scores = []\nmodel.eval()\nwith torch.no_grad():\n    for i in sub_data['image']:\n        image = read_image(os.path.join(PATH, i))\n        image = test_transform(image)\n        image = image.reshape((1, 3, 256, 256))\n        image = image.to(DEVICE)\n        \n        pred = model(image)\n        scores.append(float(pred[0][1]))\nsub_data['score'] = scores\nsub_data.head(5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T08:57:14.081414Z","iopub.execute_input":"2024-12-09T08:57:14.082230Z","iopub.status.idle":"2024-12-09T08:57:23.259492Z","shell.execute_reply.started":"2024-12-09T08:57:14.082195Z","shell.execute_reply":"2024-12-09T08:57:23.258584Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"      image     score\n0   474.jpg  0.730910\n1  1052.jpg  0.679817\n2    63.jpg  0.495740\n3  1713.jpg  0.542224\n4   116.jpg  0.593807","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image</th>\n      <th>score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>474.jpg</td>\n      <td>0.730910</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1052.jpg</td>\n      <td>0.679817</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>63.jpg</td>\n      <td>0.495740</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1713.jpg</td>\n      <td>0.542224</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>116.jpg</td>\n      <td>0.593807</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"output_dir = '/kaggle/working/'\nfile_name = 'submission.csv'\nsub_data.to_csv(os.path.join(output_dir, file_name), index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T08:57:28.158015Z","iopub.execute_input":"2024-12-09T08:57:28.158702Z","iopub.status.idle":"2024-12-09T08:57:28.165885Z","shell.execute_reply.started":"2024-12-09T08:57:28.158646Z","shell.execute_reply":"2024-12-09T08:57:28.165138Z"}},"outputs":[],"execution_count":17}]}